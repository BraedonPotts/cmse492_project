{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6212920b-b71e-4916-89bd-98632dfc1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/preprocessing/smci_preprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "def download_raw_smci(\n",
    "    ticker: str = \"SMCI\",\n",
    "    start: str = \"2017-01-01\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Download raw daily OHLCV data from Yahoo Finance.\"\"\"\n",
    "    df = yf.download(ticker, start=start, progress=False)\n",
    "    df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_ohlcv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Flatten columns if needed, drop NaNs, fix zero volumes.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Flatten columns: handle MultiIndex or tuple columns\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        if isinstance(c, tuple):\n",
    "            for part in c:\n",
    "                if part not in (None, \"\", \" \"):\n",
    "                    new_cols.append(str(part))\n",
    "                    break\n",
    "            else:\n",
    "                new_cols.append(\"col\")\n",
    "        else:\n",
    "            new_cols.append(str(c))\n",
    "    df.columns = new_cols\n",
    "\n",
    "    expected_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "    missing = [c for c in expected_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Expected columns {expected_cols}, but these are missing: {missing}.\"\n",
    "        )\n",
    "\n",
    "    df = df[expected_cols].copy()\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Drop rows with missing OHLCV\n",
    "    df = df.dropna(subset=expected_cols)\n",
    "\n",
    "    # Replace zero volume with NaN, forward-fill, then drop remaining NaNs in Volume\n",
    "    df[\"Volume\"] = df[\"Volume\"].replace(0, np.nan).ffill()\n",
    "    df = df.dropna(subset=[\"Volume\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_features(df_clean: pd.DataFrame):\n",
    "    \"\"\"Create returns, rolling means/vol, and next-day target, then df_model.\"\"\"\n",
    "    df = df_clean.copy()\n",
    "\n",
    "    # Log close\n",
    "    df[\"log_close\"] = np.log(df[\"Close\"])\n",
    "\n",
    "    # 1-day log return\n",
    "    df[\"ret_1d\"] = df[\"log_close\"].diff()\n",
    "\n",
    "    # Rolling mean returns\n",
    "    windows = [3, 5, 10, 21]\n",
    "    for w in windows:\n",
    "        df[f\"ret_mean_{w}d\"] = df[\"ret_1d\"].rolling(window=w).mean()\n",
    "\n",
    "    # 10-day rolling volatility\n",
    "    df[\"ret_vol_10d\"] = df[\"ret_1d\"].rolling(window=10).std()\n",
    "\n",
    "    # Log volume + 1-day change\n",
    "    df[\"log_vol\"] = np.log(df[\"Volume\"])\n",
    "    df[\"log_vol_chg_1d\"] = df[\"log_vol\"].diff()\n",
    "\n",
    "    # Target: next-day log return\n",
    "    df[\"target_next_log_ret\"] = df[\"log_close\"].shift(-1) - df[\"log_close\"]\n",
    "\n",
    "    feature_cols = [\n",
    "        \"ret_1d\",\n",
    "        \"ret_mean_3d\",\n",
    "        \"ret_mean_5d\",\n",
    "        \"ret_mean_10d\",\n",
    "        \"ret_mean_21d\",\n",
    "        \"ret_vol_10d\",\n",
    "        \"log_vol_chg_1d\",\n",
    "    ]\n",
    "\n",
    "    df_tmp = df[feature_cols + [\"target_next_log_ret\"]].copy()\n",
    "\n",
    "    # Drop rows with missing target and features\n",
    "    df_tmp = df_tmp[df_tmp[\"target_next_log_ret\"].notna()]\n",
    "    df_model = df_tmp.dropna().copy()\n",
    "\n",
    "    return df_model, feature_cols\n",
    "\n",
    "\n",
    "def make_time_splits(df_model: pd.DataFrame, feature_cols, train_frac=0.6, val_frac=0.2):\n",
    "    \"\"\"Create time-based 60/20/20 split for regression and classification.\"\"\"\n",
    "    X = df_model[feature_cols].values\n",
    "    y = df_model[\"target_next_log_ret\"].values\n",
    "    dates = df_model.index\n",
    "\n",
    "    n = len(df_model)\n",
    "    train_end = int(train_frac * n)\n",
    "    val_end = int((train_frac + val_frac) * n)\n",
    "\n",
    "    X_train, y_train = X[:train_end], y[:train_end]\n",
    "    X_val,   y_val   = X[train_end:val_end], y[train_end:val_end]\n",
    "    X_test,  y_test  = X[val_end:],          y[val_end:]\n",
    "\n",
    "    dates_train = dates[:train_end]\n",
    "    dates_val   = dates[train_end:val_end]\n",
    "    dates_test  = dates[val_end:]\n",
    "\n",
    "    # Classification labels: 1 if next-day return > 0\n",
    "    y_class = (df_model[\"target_next_log_ret\"].values > 0).astype(int)\n",
    "    y_class_train = y_class[:train_end]\n",
    "    y_class_val   = y_class[train_end:val_end]\n",
    "    y_class_test  = y_class[val_end:]\n",
    "\n",
    "    splits = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_class_train\": y_class_train,\n",
    "        \"y_class_val\": y_class_val,\n",
    "        \"y_class_test\": y_class_test,\n",
    "        \"dates_train\": dates_train,\n",
    "        \"dates_val\": dates_val,\n",
    "        \"dates_test\": dates_test,\n",
    "        \"train_end\": train_end,\n",
    "        \"val_end\": val_end,\n",
    "    }\n",
    "    return splits\n",
    "\n",
    "\n",
    "def load_smci_dataset(\n",
    "    ticker=\"SMCI\",\n",
    "    start=\"2017-01-01\",\n",
    "    train_frac=0.6,\n",
    "    val_frac=0.2,\n",
    "):\n",
    "    \"\"\"Full preprocessing pipeline: download, clean, build features, split.\"\"\"\n",
    "    df_raw = download_raw_smci(ticker=ticker, start=start)\n",
    "    df_clean = clean_ohlcv(df_raw)\n",
    "    df_model, feature_cols = build_features(df_clean)\n",
    "    splits = make_time_splits(df_model, feature_cols, train_frac, val_frac)\n",
    "    return df_model, feature_cols, splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fbdc8-e937-41eb-935f-686bfd0f43aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac)",
   "language": "python",
   "name": "tf-mac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
